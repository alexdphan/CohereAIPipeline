import os
from typing import List
from dotenv import load_dotenv
from fastapi import APIRouter, Query
import cohere
from pydantic import BaseModel
from cohereguard.config import co

tokenize_router = APIRouter()

class TokenizeOutput(BaseModel):
    """
    A Pydantic model representing the output data from tokenization.

    Attributes:
        tokens (List[str]): The tokens generated by the model.
    """

    tokens: List[str]


class TokenizeInput(BaseModel):
    """
    A Pydantic model representing input data for tokenization.

    Attributes:
        text (str): The text to be tokenized.
        tokenization_method (str): The tokenization method to use.
        custom_option (str): A custom option for tokenization.
    """

    text: str
    tokenization_method: str = Query("default")
    custom_option: str = Query(None)


@tokenize_router.post("/tokenize")
async def tokenize_text(input_data: TokenizeInput):
    """
    Tokenizes text using the Cohere API.

    Args:
        input_data (TokenizeInput): A Pydantic model representing input data for tokenization.

    Returns:
        A dictionary with the tokens as a list of strings.
    """
    # Add your custom tokenization logic here based on input_data.tokenization_method and input_data.custom_option
    response = co.tokenize(text=input_data.text)

    return {"tokens": response.tokens}

## Example Use Case of the Tokenize Endpoint
# Path: cohereguard/tokenize.py

# Import the necessary classes and functions from your package
# from cohereguard.tokenize import TokenizeInput, tokenize_text

# Initialize Cohere API client with your API key
# co = cohere.Client("YOUR_API_KEY")

# Define the input parameters for tokenization
# text = "The quick brown fox jumps over the lazy dog."
# tokenization_method = "custom"
# custom_option = "option_value"

# Call the tokenize_text function with the input parameters
# output = tokenize_text(TokenizeInput(text=text, tokenization_method=tokenization_method, custom_option=custom_option))

# Print the tokens
# print(output.tokens)

# =============================

## An example using the tokenization_method and custom_option parameters to customize the tokenization process. 

# For simplicity, let's assume we support two tokenization methods: "whitespace" and "punctuation". The custom_option will control whether to lowercase the tokens or not.

# python
# Copy code
# import os
# from dotenv import load_dotenv
# from fastapi import APIRouter, Query
# from typing import List
# from pydantic import BaseModel
# import re

# tokenize_router = APIRouter()

# class TokenizeOutput(BaseModel):
#     tokens: List[str]

# class TokenizeInput(BaseModel):
#     text: str
#     tokenization_method: str = Query("whitespace")
#     lowercase: bool = Query(False)

# def tokenize_whitespace(text: str, lowercase: bool) -> List[str]:
#     tokens = text.split()
#     if lowercase:
#         tokens = [token.lower() for token in tokens]
#     return tokens

# def tokenize_punctuation(text: str, lowercase: bool) -> List[str]:
#     tokens = re.findall(r'\b\w+\b', text)
#     if lowercase:
#         tokens = [token.lower() for token in tokens]
#     return tokens

# @tokenize_router.post("/tokenize")
# async def tokenize_text(input_data: TokenizeInput):
#     if input_data.tokenization_method == "whitespace":
#         tokens = tokenize_whitespace(input_data.text, input_data.lowercase)
#     elif input_data.tokenization_method == "punctuation":
#         tokens = tokenize_punctuation(input_data.text, input_data.lowercase)
#     else:
#         raise ValueError("Unsupported tokenization method.")
    
#     return {"tokens": tokens}
# In this example, I've defined two new functions tokenize_whitespace and tokenize_punctuation to handle the two supported tokenization methods. The tokenize_text function checks the tokenization_method parameter and calls the appropriate function.

# The lowercase parameter (which corresponds to the custom_option mentioned earlier) is passed to these functions as well. If lowercase is True, tokens will be converted to lowercase.

# Here's an example of how to use the tokenize_text function with the additional parameters:

# python
# Copy code
# from fastapi.testclient import TestClient
# from your_app import app

# client = TestClient(app)

# # Example 1: Tokenize using whitespace and convert tokens to lowercase
# response = client.post("/tokenize", json={"text": "The quick brown fox jumps over the lazy dog.", "tokenization_method": "whitespace", "lowercase": True})
# print(response.json()["tokens"])

# # Example 2: Tokenize using punctuation (ignores punctuation as token boundaries) and keep original case
# response = client.post("/tokenize", json={"text": "The quick brown fox jumps over the lazy dog.", "tokenization_method": "punctuation", "lowercase": False})
# print(response.json()["tokens"])
# In the examples above, the first one tokenizes the text using whitespace and converts tokens to lowercase, while the second one tokenizes using punctuation and keeps the original case.