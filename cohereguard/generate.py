import os
from dotenv import load_dotenv
from fastapi import APIRouter, Query
import cohere
from pydantic import BaseModel
from cohereguard.config import co

generate_router = APIRouter()

class GenerateOutput(BaseModel):
    """
    A Pydantic model representing the output data from text generation.

    Attributes:
        generated_text (str): The text generated by the model.
    """

    generated_text: str


class GenerateInput(BaseModel):
    """
    A Pydantic model representing input data for text generation.

    Attributes:
        model (str): The name of the generation model to use.
        prompt (str): The prompt to generate text from.
        max_tokens (int): The maximum number of tokens to generate.
        temperature (float): The temperature to use during generation.
    """

    model: str = Query("xlarge")
    prompt: str
    max_tokens: int = Query(1024)
    temperature: float = Query(1.0)


@generate_router.post("/generate")
async def generate_text(input_data: GenerateInput):
    """
    Generates text using the Cohere API.

    Args:
        input_data (GenerateInput): A Pydantic model representing input data for text generation.

    Returns:
        A dictionary with the generated text as a string.
    """
    response = co.generate(
        model=input_data.model,
        prompt=input_data.prompt,
        max_tokens=input_data.max_tokens,
        temperature=input_data.temperature,
    )

    return {"generated_text": response.generations[0].text}

## Example Use Case of the Generate Endpoint

# Import the necessary classes and functions from your package
# from cohereguard.generate import GenerateInput, generate_text

# Initialize Cohere API client with your API key
# co = cohere.Client("YOUR_API_KEY")

# Define the input parameters for text generation
# model = "xlarge"
# prompt = "The quick brown fox jumps over the lazy dog."
# max_tokens = 50
# temperature = 0.7

# Call the generate_text function with the input parameters
# output = generate_text(GenerateInput(model=model, prompt=prompt, max_tokens=max_tokens, temperature=temperature))

# Print the generated text
# print(output.generated_text)
